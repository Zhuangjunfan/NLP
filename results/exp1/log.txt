2024-12-05 19:28:08
Train Log
Epoch: 5  Batch_size: 32
learning_rate: 1e-05
best loss: 0.14845364414421575  last loss: 0.14845364414421575

other configurations:
     sequence padding length: 400
     lstm layers: 2
     lstm hidden layer size: 1024
     dropout: 0.5
     self-attention heads number: 10
     self-attention head size:30

accuracy on validation set: 0.9235955056179775